{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Classification Challenge\n",
    "\n",
    "## Challenge Overview\n",
    "Your challenge is to train a classification model to analyze wine samples and classify them by cultivar.\n",
    "\n",
    "The dataset contains 12 numeric features and 3 wine varieties (0, 1, 2).\n",
    "Your goal is to achieve >95% Recall performance.\n",
    "\n",
    "## Dataset Citation\n",
    "Originally collected by Forina, M. et al. from the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset from sklearn\n",
    "wine = load_wine()\n",
    "data = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "data['target'] = wine.target\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nTarget classes: {wine.target_names}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 rows\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "data['target'].value_counts().plot(kind='bar')\n",
    "plt.title('Wine Class Distribution')\n",
    "plt.xlabel('Wine Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = data.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train_scaled.shape}\")\n",
    "print(f\"Test set size: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'recall': recall\n",
    "    }\n",
    "    \n",
    "    print(f\"Recall Score: {recall:.4f}\")\n",
    "    print(f\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "recall_scores = {name: results[name]['recall'] for name in results}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(recall_scores.keys(), recall_scores.values())\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='Target Recall (95%)')\n",
    "plt.title('Model Recall Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "for i, v in enumerate(recall_scores.values()):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(recall_scores, key=recall_scores.get)\n",
    "best_model = results[best_model_name]['model']\n",
    "best_recall = recall_scores[best_model_name]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} with Recall: {best_recall:.4f}\")\n",
    "print(f\"Target achieved: {'Yes' if best_recall > 0.95 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Detailed Analysis of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='recall_macro')\n",
    "print(f\"\\nCross-Validation Recall Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Recall: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final Challenge - Predict New Wine Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New wine samples to classify\n",
    "sample_1 = [13.72, 1.43, 2.5, 16.7, 108, 3.4, 3.67, 0.19, 2.04, 6.8, 0.89, 2.87, 1285]\n",
    "sample_2 = [12.37, 0.94, 1.36, 10.6, 88, 1.98, 0.57, 0.28, 0.42, 1.95, 1.05, 1.82, 520]\n",
    "\n",
    "# Convert to DataFrame with proper column names\n",
    "new_samples = pd.DataFrame([sample_1, sample_2], columns=X.columns)\n",
    "\n",
    "print(\"New wine samples to classify:\")\n",
    "print(new_samples)\n",
    "\n",
    "# Scale the new samples using the same scaler\n",
    "new_samples_scaled = scaler.transform(new_samples)\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_model.predict(new_samples_scaled)\n",
    "probabilities = best_model.predict_proba(new_samples_scaled) if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "print(f\"\\n=== Predictions using {best_model_name} ===\")\n",
    "for i, (pred, sample) in enumerate(zip(predictions, [sample_1, sample_2])):\n",
    "    print(f\"\\nSample {i+1}: {wine.target_names[pred]}\")\n",
    "    if probabilities is not None:\n",
    "        print(f\"Confidence: {probabilities[i][pred]:.3f}\")\n",
    "        print(f\"All probabilities: {dict(zip(wine.target_names, probabilities[i]))}\")\n",
    "\n",
    "print(f\"\\nâœ… Challenge Complete! Best model recall: {best_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully implements a wine classification solution that:\n",
    "1. Loads and explores the wine dataset\n",
    "2. Preprocesses the data with proper scaling\n",
    "3. Tests multiple classification models\n",
    "4. Achieves the target >95% recall performance\n",
    "5. Makes predictions on new wine samples\n",
    "\n",
    "The solution demonstrates proper machine learning practices including data exploration, model comparison, and performance evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}